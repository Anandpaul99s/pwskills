{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2d7ed9c-91f0-4838-ae40-b7f1dc396574",
   "metadata": {},
   "source": [
    "## Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cfe12b-b1ff-4a17-9a31-0eae78f169a4",
   "metadata": {},
   "source": [
    "Ans) Web scraping is the process of extracting data from websites using automated tools. It involves using a program to send requests to a website and then parsing the HTML or XML code to extract the desired information. Web scraping is used to collect data from websites that do not offer an API or other means of accessing their data programmatically. Three areas where web scraping is commonly used include:\n",
    "\n",
    "1. E-commerce: Web scraping is often used by e-commerce companies to collect data about their competitors' prices, products, and promotions.\n",
    "2. Market research: Web scraping can be used to collect data on consumer sentiment, news articles, and other information that can be used for market research.\n",
    "3. Social media: Web scraping is used to collect data from social media platforms like Twitter and Facebook, allowing researchers to analyze trends, sentiment, and other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca5231a-30d5-47cd-8153-5b493b355126",
   "metadata": {},
   "source": [
    "## Q2. What are the different methods used for Web Scraping?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f041405-e7eb-4b07-b54d-8a5289338621",
   "metadata": {},
   "source": [
    "Ans) There are several methods used for web scraping, including:\n",
    "\n",
    "Parsing HTML and XML: This involves using libraries like Beautiful Soup to parse the HTML or XML code of a website and extract the desired data.\n",
    "1. Using web scraping tools: There are several web scraping tools available, like Scrapy and BeautifulSoup, that can be used to automate the web scraping process.\n",
    "2. Using APIs: Some websites offer APIs that can be used to access their data programmatically.\n",
    "3. Using browser extensions: There are several browser extensions like Web Scraper and Data Miner that can be used to extract data from websites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d85340-e755-4bad-ba10-4d5a4b2b7697",
   "metadata": {},
   "source": [
    "## Q3. What is Beautiful Soup? Why is it used?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace8603-3efb-4ddd-a6c6-e8640d37f4f1",
   "metadata": {},
   "source": [
    "Ans) Beautiful Soup is a Python library used for web scraping. It is used to parse HTML or XML code and extract the desired data. Beautiful Soup provides a set of functions and methods that make it easy to navigate and search the HTML or XML tree structure of a webpage. Beautiful Soup is used for web scraping because it simplifies the process of parsing HTML and XML code and extracting data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb35cb-b036-41f5-9b4e-171d378a7cbe",
   "metadata": {},
   "source": [
    "## Q4. Why is flask used in this Web Scraping project?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fbece6-13a9-4837-88ed-92816219f093",
   "metadata": {},
   "source": [
    "Ans) Flask is a Python web framework that is used to create web applications. Flask is used in this web scraping project to create a RESTful API that exposes the scraped data. The Flask API allows the scraped data to be easily accessed and consumed by other applications. Flask is lightweight and easy to use, making it a good choice for creating small web applications like the one in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18125600-cc8d-41be-85b4-297bb0ab1f06",
   "metadata": {},
   "source": [
    "## Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5262f-0d00-4fa5-9f57-57e6dc839ea9",
   "metadata": {},
   "source": [
    "Ans) The AWS services used in this project include:\n",
    "\n",
    "1. Amazon EC2: Amazon Elastic Compute Cloud (EC2) is used to host the web application and API. EC2 provides scalable compute capacity in the cloud.\n",
    "2. Amazon RDS: Amazon Relational Database Service (RDS) is used to store the scraped data. RDS is a managed database service that makes it easy to set up, operate, and scale a relational database in the cloud.\n",
    "3. Amazon S3: Amazon Simple Storage Service (S3) is used to store the images that are scraped from the website. S3 is a highly scalable object storage service that provides secure, durable, and highly available storage for objects.\n",
    "4. Amazon CloudWatch: Amazon CloudWatch is used to monitor the performance of the web application and API. CloudWatch provides monitoring for resources and applications in the cloud, enabling users to collect and track metrics, collect and monitor log files, and set alarms.\n",
    "5. Amazon Route 53: Amazon Route 53 is used to manage the domain name system (DNS) for the web application. Route 53 is a highly available and scalable DNS service that provides routing for traffic to internet resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
